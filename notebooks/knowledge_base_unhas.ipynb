{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e219faca",
   "metadata": {},
   "source": [
    "Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b7b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (26.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: numpy in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: requests in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 3)) (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 4)) (4.14.2)\n",
      "Requirement already satisfied: lxml in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: pdfplumber in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 6)) (0.11.9)\n",
      "Requirement already satisfied: tqdm in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: langchain-text-splitters in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: pytesseract in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 9)) (0.3.13)\n",
      "Requirement already satisfied: pdf2image in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: pillow in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from -r ../requirements.txt (line 11)) (11.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from pandas->-r ../requirements.txt (line 1)) (2025.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from requests->-r ../requirements.txt (line 3)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from requests->-r ../requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from requests->-r ../requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from requests->-r ../requirements.txt (line 3)) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from beautifulsoup4->-r ../requirements.txt (line 4)) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from beautifulsoup4->-r ../requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: pdfminer.six==20251230 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from pdfplumber->-r ../requirements.txt (line 6)) (20251230)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from pdfplumber->-r ../requirements.txt (line 6)) (5.3.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from pdfminer.six==20251230->pdfplumber->-r ../requirements.txt (line 6)) (41.0.7)\n",
      "Requirement already satisfied: colorama in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from tqdm->-r ../requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langchain-text-splitters->-r ../requirements.txt (line 8)) (1.2.8)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (0.6.8)\n",
      "Requirement already satisfied: packaging>=23.2.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: xxhash>=3.0.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (0.25.0)\n",
      "Requirement already satisfied: anyio in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (0.4.2)\n",
      "Requirement already satisfied: cffi>=1.12 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber->-r ../requirements.txt (line 6)) (1.17.1)\n",
      "Requirement already satisfied: pycparser in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber->-r ../requirements.txt (line 6)) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from python-dateutil>=2.8.2->pandas->-r ../requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in C:\\Users\\syhar\\AppData\\Roaming\\Python\\Python312\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters->-r ../requirements.txt (line 8)) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "'''\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install pdfplumber\n",
    "%pip install tqdm\n",
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "%pip install lxml\n",
    "%pip install langchain-text-splitters\n",
    "'''\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6297839",
   "metadata": {},
   "source": [
    "PHASE A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9e9a2",
   "metadata": {},
   "source": [
    "PHASE 1A: Scraping News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e85216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mencoba mencari dengan class 'jet-smart-tiles__box-link'...\n",
      "Ditemukan 33 elemen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Berita: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Berhasil mengambil data!\n",
      "                                               title  \\\n",
      "0  Mau Kuliah di Unhas? Ayo. Hadiri Unhas Open Da...   \n",
      "1  22 Lulusan Terbaik Unhas Raih Penghargaan, Dom...   \n",
      "2  Rektor Unhas Tekankan Penguatan Budaya Ilmiah ...   \n",
      "3  Sebanyak 1.365 Wisudawan Baru Pada Wisuda Unha...   \n",
      "4  Komisi Nasional Disabilitas Berkunjung ke Unha...   \n",
      "\n",
      "                                              source  \n",
      "0  https://www.unhas.ac.id/mau-kuliah-di-unhas-ay...  \n",
      "1  https://www.unhas.ac.id/22-lulusan-terbaik-unh...  \n",
      "2  https://www.unhas.ac.id/rektor-unhas-tekankan-...  \n",
      "3  https://www.unhas.ac.id/sebanyak-1-365-wisudaw...  \n",
      "4  https://www.unhas.ac.id/komisi-nasional-disabi...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "def scrape_unhas_smart_tiles():\n",
    "    url = \"https://www.unhas.ac.id/berita/?lang=id\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers,  timeout=10)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    articles = soup.find_all('a', class_='jet-smart-tiles__box-link')\n",
    "    \n",
    "    news_list = []\n",
    "    \n",
    "    print(f\"Mencoba mencari dengan class 'jet-smart-tiles__box-link'...\")\n",
    "    print(f\"Ditemukan {len(articles)} elemen.\")\n",
    "\n",
    "    for link_tag in tqdm(articles, desc=\"Scraping Berita\"):\n",
    "        title = link_tag.get('aria-label')\n",
    "        link = link_tag.get('href')\n",
    "\n",
    "        if title and link:\n",
    "            news_list.append({\n",
    "                'title': title,\n",
    "                'source': link\n",
    "            })\n",
    "            \n",
    "    return news_list\n",
    "\n",
    "hasil = scrape_unhas_smart_tiles()\n",
    "\n",
    "if hasil:\n",
    "    df = pd.DataFrame(hasil)\n",
    "    print(\"\\nBerhasil mengambil data!\")\n",
    "    print(df.head())\n",
    "    df.to_csv('../data/processed/dataset_berita_unhas.csv', index=False)\n",
    "else:\n",
    "    print(\"\\nError saat mengambil data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02f49fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai mengambil isi berita satu per satu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:42<00:00,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SELESAI! ---\n",
      "                                               title  \\\n",
      "0  Mau Kuliah di Unhas? Ayo. Hadiri Unhas Open Da...   \n",
      "1  22 Lulusan Terbaik Unhas Raih Penghargaan, Dom...   \n",
      "2  Rektor Unhas Tekankan Penguatan Budaya Ilmiah ...   \n",
      "3  Sebanyak 1.365 Wisudawan Baru Pada Wisuda Unha...   \n",
      "4  Komisi Nasional Disabilitas Berkunjung ke Unha...   \n",
      "\n",
      "                                             content  \n",
      "0  Universitas Hasanuddin mengundang para siswa S...  \n",
      "1  Universitas Hasanuddin menggelar prosesi Wisud...  \n",
      "2  Rektor Unhas, Prof. Dr. Ir. Jamaluddin Jompa, ...  \n",
      "3  Universitas Hasanuddin kembali menyelenggaraka...  \n",
      "4  Universitas Hasanuddin melalui Pusat Disabilit...  \n",
      "Data lengkap disimpan ke 'dataset_unhas_lengkap.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# df = pd.DataFrame(hasil) \n",
    "# Kalau sudah hilang, load dari CSV: df = pd.read_csv('../data/processed/dataset_berita_unhas.csv')\n",
    "\n",
    "def get_content(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=20)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        content_div = soup.find('div', class_='elementor-widget-theme-post-content')\n",
    "        \n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            full_text = ' '.join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "            return full_text\n",
    "        else:\n",
    "            # Cadangan: struktur standar WordPress\n",
    "            backup_div = soup.find('div', class_='entry-content')\n",
    "            if backup_div:\n",
    "                paragraphs = backup_div.find_all('p')\n",
    "                return ' '.join([p.text.strip() for p in paragraphs])\n",
    "            \n",
    "            return \"\" \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "print(\"Mulai mengambil isi berita satu per satu...\")\n",
    "\n",
    "full_contents = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    url = row['source']\n",
    "    \n",
    "    content = get_content(url)\n",
    "    full_contents.append(content)\n",
    "    \n",
    "    # agar tidak dianggap ddos\n",
    "    time.sleep(1)\n",
    "    \n",
    "df['content'] = full_contents\n",
    "df['type'] = 'berita'\n",
    "\n",
    "# filter hasil\n",
    "df_filter = df[df['content'].str.len() > 50]\n",
    "print(\"\\n--- SELESAI! ---\")\n",
    "print(df_filter[['title', 'content']].head())\n",
    "df_filter.to_csv('../data/processed/dataset_berita_unhas_lengkap.csv', index=False)\n",
    "print(\"Data lengkap disimpan ke 'dataset_unhas_lengkap.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520e4f1",
   "metadata": {},
   "source": [
    "PHASE 2A: Cleaning News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80058811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang membersihkan teks berita (Regex)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Content: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:00<00:00, 2565.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang memfilter data yang terlalu pendek...\n",
      "   -> Data awal: 33 baris\n",
      "   -> Data dibuang: 0 baris (Terlalu pendek/kosong)\n",
      "   -> Data akhir: 33 baris\n",
      "\n",
      "SUKSES! Dataset bersih tersimpan di: ../data/processed/dataset_berita_unhas_clean.csv\n",
      "\n",
      "--- Contoh 1 Data Teratas ---\n",
      "Judul  : Mau Kuliah di Unhas? Ayo. Hadiri Unhas Open Day 2026\n",
      "Konten (Akhir): ... i daerah. Bagi calon peserta yang ingin mengikuti kegiatan ini, proses pendaftaran dapat dilakukan melalui tautan https://s.unhas.ac.id/RegistUOD2026.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. LOAD DATASET MENTAH\n",
    "df = pd.read_csv(\"../data/processed/dataset_berita_unhas_lengkap.csv\")\n",
    "\n",
    "# 2. DEFINISI FUNGSI PEMBERSIH (REGEX)\n",
    "def clean_news_text(text):\n",
    "    # a. Pastikan input adalah string (jaga-jaga ada nilai NaN/Null)\n",
    "    text = str(text)\n",
    "    \n",
    "    # b. Hapus Newline, Tab, Return (Ganti dengan 1 spasi)\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    \n",
    "    # c. Hapus pola \"Baca Juga: ...\"\n",
    "    text = re.sub(r'Baca [Jj]uga.*?(?=\\.|$)', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # d. Hapus inisial penulis/fotografer di dalam kurung\n",
    "    # Contoh: (*/mir), (*/dhs), (Humas Unhas), (img: dok)\n",
    "    text = re.sub(r'\\s*\\([^)]*?Unhas.*?\\)', '', text, flags=re.IGNORECASE) # Spesifik Unhas\n",
    "    text = re.sub(r'\\s*\\(\\*/.*?\\)', '', text) # Pola inisial wartawan (*/...)\n",
    "    \n",
    "    # e. Hapus \"Editor :\" atau \"Penulis :\" di akhir berita sampai habis\n",
    "    text = re.sub(r'(Editor|Penulis)\\s*[:|].*?$', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # f. Hapus Spasi Berlebih (Double space jadi single space)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 3. TERAPKAN PEMBERSIHAN\n",
    "print(\"Sedang membersihkan teks berita (Regex)...\")\n",
    "tqdm.pandas(desc=\"Cleaning Content\")\n",
    "df['content'] = df['content'].progress_apply(clean_news_text)\n",
    "\n",
    "# 4. FILTERING KUALITAS (HAPUS BARIS SAMPAH)\n",
    "print(\"Sedang memfilter data yang terlalu pendek...\")\n",
    "jumlah_awal = len(df)\n",
    "df_clean = df[df['content'].str.len() > 50].copy()\n",
    "jumlah_akhir = len(df_clean)\n",
    "dibuang = jumlah_awal - jumlah_akhir\n",
    "\n",
    "print(f\"   -> Data awal: {jumlah_awal} baris\")\n",
    "print(f\"   -> Data dibuang: {dibuang} baris (Terlalu pendek/kosong)\")\n",
    "print(f\"   -> Data akhir: {jumlah_akhir} baris\")\n",
    "\n",
    "# 5. SIMPAN HASIL AKHIR\n",
    "output_file = '../data/processed/dataset_berita_unhas_clean.csv'\n",
    "df_clean.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nSUKSES! Dataset bersih tersimpan di: {output_file}\")\n",
    "\n",
    "# 6. CEK CONTOH DATA (PREVIEW)\n",
    "print(\"\\n--- Contoh 1 Data Teratas ---\")\n",
    "print(\"Judul  :\", df_clean.iloc[0]['title'])\n",
    "print(\"Konten (Akhir): ...\", df_clean.iloc[0]['content'][-150:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e301eb8",
   "metadata": {},
   "source": [
    "PHASE B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d9590",
   "metadata": {},
   "source": [
    "PHASE 1B: Extract PDF to TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd844830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai OCR untuk 5 dokumen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Proses OCR PDF:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Mengonversi PDF ke Gambar & OCR: ACADEMIC_REGULATION_cfecbc8cb1.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Proses OCR PDF:  20%|‚ñà‚ñà        | 1/5 [01:02<04:08, 62.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úÖ Selesai: 70492 karakter.\n",
      "-> Mengonversi PDF ke Gambar & OCR: ATURAN REKTOR - Pedoman Tata Cara Penyimpanan Karya Ilmiah pada Repositori Universitas Hasanuddin 2024-2.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Proses OCR PDF:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [01:15<01:39, 33.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úÖ Selesai: 9519 karakter.\n",
      "-> Mengonversi PDF ke Gambar & OCR: ATURAN REKTOR -- TTG PEMANFAATAN KOLEKSI PERPUSTAKAAN UNHAS. 2025. fix.-1.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Proses OCR PDF:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [01:28<00:48, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úÖ Selesai: 9668 karakter.\n",
      "-> Mengonversi PDF ke Gambar & OCR: SK Rektor tentang Peraturan Pemanfaatan Koleksi Perpustakaan Unhas (2).pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Proses OCR PDF:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:37<00:18, 18.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úÖ Selesai: 7258 karakter.\n",
      "-> Mengonversi PDF ke Gambar & OCR: SOP KEANGGOTAAN.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Proses OCR PDF: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:42<00:00, 20.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úÖ Selesai: 2764 karakter.\n",
      "\n",
      "SUKSES! Data tersimpan di 'dataset_peraturan_unhas.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Lokasi Tesseract.exe\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# 2. Lokasi Poppler (Folder bin)\n",
    "POPPLER_PATH = r'C:\\poppler-25.12.0\\Library\\bin' \n",
    "\n",
    "# 3. Folder Data\n",
    "FOLDER_PDF = '../data/pdf/'\n",
    "OUTPUT_CSV = '../data/processed/dataset_peraturan_unhas.csv'\n",
    "\n",
    "def clean_ocr_text(text):\n",
    "    text = re.sub(r'\\n', ' ', text)       # Hapus enter\n",
    "    text = re.sub(r'\\s+', ' ', text)      # Hapus spasi ganda\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,/:;()%\\- ]', '', text) # Hapus karakter aneh\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_with_ocr(pdf_path):\n",
    "    print(f\"-> Mengonversi PDF ke Gambar & OCR: {os.path.basename(pdf_path)}...\")\n",
    "    full_text = \"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Ubah PDF jadi List Gambar (Per Halaman)\n",
    "        images = convert_from_path(pdf_path, poppler_path=POPPLER_PATH)\n",
    "        \n",
    "        # 2. Baca tiap halaman pakai Tesseract\n",
    "        for i, image in enumerate(images):\n",
    "            page_text = pytesseract.image_to_string(image, lang='eng') \n",
    "            full_text += page_text + \" \"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error OCR: {e}\")\n",
    "        return \"\"\n",
    "        \n",
    "    return full_text\n",
    "\n",
    "data_pdf = []\n",
    "\n",
    "if not os.path.exists(FOLDER_PDF):\n",
    "    print(f\"Folder '{FOLDER_PDF}' tidak ditemukan.\")\n",
    "else:\n",
    "    files = [f for f in os.listdir(FOLDER_PDF) if f.lower().endswith('.pdf')]\n",
    "    print(f\"Mulai OCR untuk {len(files)} dokumen...\")\n",
    "\n",
    "    for filename in tqdm(files, desc=\"Proses OCR PDF\"):\n",
    "        filepath = os.path.join(FOLDER_PDF, filename)\n",
    "        \n",
    "        # Proses OCR \n",
    "        raw_text = extract_text_with_ocr(filepath)\n",
    "        \n",
    "        # Cleaning\n",
    "        clean_text = clean_ocr_text(raw_text)\n",
    "        \n",
    "        if len(clean_text) > 50:\n",
    "            data_pdf.append({\n",
    "                'title': filename,\n",
    "                'source': f\"Dokumen PDF ({filename})\",\n",
    "                'content': clean_text,\n",
    "                'type': 'pdf'\n",
    "            })\n",
    "            print(f\"      ‚úÖ Selesai: {len(clean_text)} karakter.\")\n",
    "\n",
    "    # Simpan\n",
    "    if data_pdf:\n",
    "        df = pd.DataFrame(data_pdf)\n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "        print(f\"\\nSUKSES! Data tersimpan di 'dataset_peraturan_unhas.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92606ef0",
   "metadata": {},
   "source": [
    "PHASE 2B: Cleaning PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ebbcc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai membersihkan typo dan artifact...\n",
      "\n",
      "--- CONTOH PERBAIKAN ---\n",
      "SEBELUM: - SKEPUTUSAN mt REKTOR UNIVERSITAS HASANUDDIN NOMOR : 609/114/P/2007  TENTANG : PERATURAN PEMANFAATA\n",
      "SESUDAH: - KEPUTUSAN REKTOR UNIVERSITAS HASANUDDIN NOMOR : 609/114/P/2007 TENTANG : PERATURAN PEMANFAATAN KOL\n",
      "\n",
      "‚úÖ Data bersih tersimpan di 'dataset_peraturan_unhas_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load Data \n",
    "input_file = '../data/processed/dataset_peraturan_unhas.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 2. Definisi Kamus Typo \n",
    "KAMUS_TYPO = {\n",
    "    \"SKEPUTUSAN\": \"KEPUTUSAN\",\n",
    "    \"SKEP\": \"KEP\",\n",
    "    \"mt \": \"\",      # Hapus 'mt' (biasanya noise logo)\n",
    "    \"ae \": \"a. \",   # Perbaiki bullet point a\n",
    "    \"bs \": \"b. \",   # Perbaiki bullet point b\n",
    "    \"cs \": \"c. \",   # Perbaiki bullet point c\n",
    "    \"ds \": \"d. \",\n",
    "    \"Menimbang :\": \"\\nMenimbang :\", # Kasih enter biar rapi\n",
    "    \"Mengingat :\": \"\\nMengingat :\",\n",
    "    \"Memutuskan :\": \"\\nMemutuskan :\",\n",
    "    \"MEMUTUSKAN\": \"\\nMEMUTUSKAN\",\n",
    "    \"BABI\": \"BAB I\",\n",
    "    \"BAB Ill\": \"BAB III\",\n",
    "    \"BAB Il\": \"BAB II\",\n",
    "}\n",
    "\n",
    "def advanced_cleaning(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # 1. Hapus Metadata Web\n",
    "    text = re.sub(r'Baca [Jj]uga.*?(?=\\.|$)', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s*\\([^)]*?Unhas.*?\\)', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(Editor|Penulis)\\s*[:|].*?$', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 2. Perbaikan Typo OCR \n",
    "    for salah, benar in KAMUS_TYPO.items():\n",
    "        text = text.replace(salah, benar)\n",
    "        \n",
    "    # 3. Fix Bullet Points\n",
    "    text = re.sub(r'(?m)^\\s*([a-z])\\s+', r'\\1. ', text)\n",
    "\n",
    "    # 4. Final Trim (Hanya hapus spasi/tab berlebih)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text) \n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n', text) \n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# 3. Eksekusi\n",
    "print(\"Mulai membersihkan typo dan artifact...\")\n",
    "content = df['content'].copy()\n",
    "df['content'] = df['content'].apply(advanced_cleaning)\n",
    "\n",
    "# 4. Cek Hasil\n",
    "idx = content.str.contains(\"SKEPUTUSAN\", na=False)\n",
    "if idx.any():\n",
    "    target_idx = idx[idx].index[0]\n",
    "    print(\"\\n--- CONTOH PERBAIKAN ---\")\n",
    "    print(\"SEBELUM:\", content[target_idx][:100]) \n",
    "    print(\"SESUDAH:\", df.loc[target_idx, 'content'][:100])\n",
    "else:\n",
    "    print(\"\\nTidak menemukan 'SKEPUTUSAN'.\")\n",
    "\n",
    "# 5. Simpan Hasil Akhir\n",
    "df.to_csv('../data/processed/dataset_peraturan_unhas_clean.csv', index=False)\n",
    "print(\"\\n‚úÖ Data bersih tersimpan di 'dataset_peraturan_unhas_clean.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ba03e",
   "metadata": {},
   "source": [
    "PHASE 3: MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed018a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang menggabungkan dataset Berita dan PDF...\n",
      "‚úÖ Data Berita: 33 baris\n",
      "‚úÖ Data PDF: 5 dokumen\n",
      "\n",
      "üéâ TOTAL DATASET: 38 baris.\n",
      "Tersimpan di 'dataset_master_unhas.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"Sedang menggabungkan dataset Berita dan PDF...\")\n",
    "\n",
    "# 1. Load Data Berita \n",
    "try:\n",
    "    df_berita = pd.read_csv('../data/processed/dataset_berita_unhas_clean.csv')\n",
    "    print(f\"‚úÖ Data Berita: {len(df_berita)} baris\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Data Berita tidak ditemukan! Jalankan Phase 1 & 2 dulu.\")\n",
    "    df_berita = pd.DataFrame()\n",
    "\n",
    "# 2. Load Data PDF \n",
    "try:\n",
    "    df_pdf = pd.read_csv('../data/processed/dataset_peraturan_unhas.csv')\n",
    "    print(f\"‚úÖ Data PDF: {len(df_pdf)} dokumen\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ÑπÔ∏è Data PDF belum ada (Mungkin folder kosong). Skip.\")\n",
    "    df_pdf = pd.DataFrame()\n",
    "\n",
    "# 3. Gabungkan \n",
    "if not df_pdf.empty:    \n",
    "    # Pilih kolom yang sama saja\n",
    "    cols = ['title', 'source', 'content', 'type']\n",
    "    df_combined = pd.concat([df_berita[cols], df_pdf[cols]], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_berita\n",
    "\n",
    "# 4. Simpan Master Dataset\n",
    "output_master = '../data/processed/dataset_master_unhas.csv'\n",
    "df_combined.to_csv(output_master, index=False)\n",
    "\n",
    "print(f\"\\nüéâ TOTAL DATASET: {len(df_combined)} baris.\")\n",
    "print(f\"Tersimpan di 'dataset_master_unhas.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ad896",
   "metadata": {},
   "source": [
    "PHASE 4: CHUNKING & EXPORT TO JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb97722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membaca data dari 'dataset_master_unhas.csv'...\n",
      "Sedang melakukan chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [00:00<00:00, 9308.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ SELESAI! Data siap RAG.\n",
      "   - Total Dokumen Asli : 38\n",
      "   - Total Chunks (Potongan) : 241\n",
      "   - Tersimpan di : ../output/knowledge_base_unhas.jsonl\n",
      "\n",
      "--- CONTOH CHUNK (JSONL) ---\n",
      "{\n",
      "  \"id\": \"berita_0_0\",\n",
      "  \"title\": \"Mau Kuliah di Unhas? Ayo. Hadiri Unhas Open Day 2026\",\n",
      "  \"source\": \"https://www.unhas.ac.id/mau-kuliah-di-unhas-ayo-hadiri-unhas-open-day-2026/?lang=id\",\n",
      "  \"text\": \"Universitas Hasanuddin mengundang para siswa Sekolah Menengah Atas (SMA), khususnya yang saat ini duduk di kelas XII, untuk menghadiri Unhas Open Day (UOD) 2026. Kegiatan ini akan berlangsung selama 2 hari, yaitu Sabtu dan Minggu, 7 dan 8 Februari 2026, di GOR JK Arenatorium, mulai pukul 08.00 WITA. UOD 2026 merupakan ajang memperkenalkan kampus secara lebih dekat kepada calon mahasiswa dan masyarakat luas. Wakil Rektor Bidang Akademik dan Kemahasiswaan, Prof. drg. Muhammad Ruslin, M.Kes., Ph.D., Sp.BM(K), menjelaskan UOD 2026 merupakan bagian penting dari komitmen Unhas dalam menghadirkan layanan informasi pendidikan tinggi yang terbuka, akurat, dan mudah diakses oleh calon mahasiswa. \\u201cKegiatan tahunan ini kami rancang sebagai ruang komunikasi langsung antara kampus dan calon mahasiswa\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load Data Gabungan (Merged CSV)\n",
    "input_file = '../data/processed/dataset_master_unhas.csv' \n",
    "print(f\"Membaca data dari 'dataset_master_unhas.csv'...\")\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 2. Konfigurasi Chunker\n",
    "# - chunk_size=1000: Setiap potongan sekitar 1000 karakter (cukup untuk 1-2 paragraf)\n",
    "# - chunk_overlap=200: Ada irisan 200 karakter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Prioritas pemotongan\n",
    ")\n",
    "\n",
    "# 3. Proses Chunking\n",
    "print(\"Sedang melakukan chunking...\")\n",
    "jsonl_data = []\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    # Ambil teks\n",
    "    original_text = str(row['content']) # Pastikan string\n",
    "    source= row['source']\n",
    "    title = row['title']\n",
    "    \n",
    "    # Lakukan pemotongan\n",
    "    chunks = text_splitter.split_text(original_text)\n",
    "    \n",
    "    # Simpan setiap potongan\n",
    "    for i, chunk_text in enumerate(chunks):\n",
    "        entry = {\n",
    "            \"id\": f\"{row['type']}_{index}_{i}\",          \n",
    "            \"title\": title,\n",
    "            \"source\": source,\n",
    "            \"text\": chunk_text              \n",
    "        }\n",
    "        jsonl_data.append(entry)\n",
    "\n",
    "# 4. Simpan ke JSONL \n",
    "output_file = '../output/knowledge_base_unhas.jsonl'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for entry in jsonl_data:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"\\nüéâ SELESAI! Data siap RAG.\")\n",
    "print(f\"   - Total Dokumen Asli : {len(df)}\")\n",
    "print(f\"   - Total Chunks (Potongan) : {len(jsonl_data)}\")\n",
    "print(f\"   - Tersimpan di : {output_file}\")\n",
    "\n",
    "# 5. Preview Hasil\n",
    "print(\"\\n--- CONTOH CHUNK (JSONL) ---\")\n",
    "print(json.dumps(jsonl_data[0], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
